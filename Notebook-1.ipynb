{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d8b62d",
   "metadata": {},
   "source": [
    "# Group Name & numer: DLNK, #26\n",
    "## Group members:\n",
    "Natiq Khan (nak9135@nyu.edu)\\\n",
    "David Lopez (dld388@nyu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23634fe",
   "metadata": {},
   "source": [
    "Welcome to our project! For our project, we will be analysing and attempting to predict underdog-wins in the NFL using machine learning techniques. We use a variety of data sources, feature engineering and prediction models to accomplish this goal.\\\n",
    "We define an `underdog-win` as a situation where a team with an unfavorable spread (i.e., betting odds) takes the win in a game.\\\n",
    "We acknowledge the possibility that our model fails to reliably predict underdogs in a way that is better than the betting lines, but regardless, this presents a great exercise about data analysis, machine learning modeling as well as industry coding practices. It actually proved useful to have the Vegas line as a point of reference: if our model is making the same predictions as the vegas line, then that would be signify our code is mostly on the right track :)\\\n",
    "\n",
    "**DISCLAIMER**: *We are not doing this for gambling purposes, but rather to try and uncover patterns and trends previously undetected. This project is motivated purely by curiosity and has zero monetary incentives.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1e6e2",
   "metadata": {},
   "source": [
    "Our project is divided into 4 notebooks, each serving its own unique function:\n",
    "1. **→Notebook-1: Data retrieval and loading**\n",
    "2. Notebook-2: Data cleaning and standardizing\n",
    "3. Notebook-3: Exploratory analysis and visualizations\n",
    "4. Notebook-4: Machine Learning and predictions\n",
    "\n",
    "We made each notebook in keeping with the principles of **modularization and testing**. As well as **abstraction**, which made it easier to share our work between members seamlessly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b282d5",
   "metadata": {},
   "source": [
    "# 1.1 Configuring environment\n",
    "While we had access to data between 1999 and 2025, there have been changes in team names and major rule overhauls over the years which make it very difficult to carry out analysis over this complete timeline. Thus, we chose the largest consistent interval in NFL history, which is **2005-2015** for our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e991be",
   "metadata": {},
   "source": [
    "### We begin by downloading the CSV files for the play-by-play datasets from the `nflverse` repository: (https://github.com/nflverse/nflverse-data/releases/tag/pbp). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c1bc7a",
   "metadata": {},
   "source": [
    "General overview of what the code below does:\n",
    "1. Import necessary libraries\n",
    "2. Download NFL play-by-play CSV files from the nflverse GitHub release, restricted to 2005–2015.\n",
    "3. Sequential execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026dc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create folders and save files on system\n",
    "import os\n",
    "# For parsing and selecting specific files\n",
    "import re\n",
    "# In order to make web requests\n",
    "import requests\n",
    "# For type hints, to make code easier to read and debug\n",
    "from typing import Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff16ed",
   "metadata": {},
   "source": [
    "In order to keep our code modular, we made reusable functions which can easily be passed new parameters in case our requirements change in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fb40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OWNER = \"nflverse\"\n",
    "REPO = \"nflverse-data\"\n",
    "TAG = \"pbp\"  # GitHub release tag where PBP data resides\n",
    "DOWNLOAD_DIR = \"pbp_csv\"\n",
    "API_URL = f\"https://api.github.com/repos/{OWNER}/{REPO}/releases/tags/{TAG}\"\n",
    "\n",
    "# Only download files whose filename contains a year in this interval:\n",
    "START_SEASON = 2005\n",
    "END_SEASON = 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf320c",
   "metadata": {},
   "source": [
    "# 1.2 Functions definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea4f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions:\n",
    "\n",
    "# We added Universal timeout limit for all functions using internet\n",
    "# This handled almost all of our test cases\n",
    "DEFAULT_TIMEOUT = 20  # seconds\n",
    "\n",
    "def extract_year_from_filename(name: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Extract a 4-digit year from a filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Filename to inspect.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int or None\n",
    "        Four-digit year if found, otherwise None.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"(\\d{4})\", name)\n",
    "    if not match:\n",
    "        return None\n",
    "    return int(match.group(1))\n",
    "\n",
    "\n",
    "def fetch_release(tag: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch metadata for a GitHub release by tag.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : str\n",
    "        Release tag to query (e.g., 'pbp').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        JSON metadata describing the release, including assets.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If the GitHub request fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(API_URL, timeout=DEFAULT_TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        raise RuntimeError(f\"Failed to fetch release '{tag}' from GitHub: {e}\") from e\n",
    "\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "def download_asset(url: str, out_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a file from a URL and save it locally in streamed chunks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Direct browser_download_url from the GitHub release.\n",
    "\n",
    "    out_path : str\n",
    "        Output path for the downloaded file.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If the download operation encounters a network or file error.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=DEFAULT_TIMEOUT) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "    except (requests.RequestException, OSError) as e:\n",
    "        raise RuntimeError(f\"Failed to download asset from {url}: {e}\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8058aa",
   "metadata": {},
   "source": [
    "# 1.3 Function calls\n",
    "Now that we have our utility functions ready, we can begin the html requests and retrieve our data files. The code below does the following:\n",
    "1. Fetch the metadata, so it knows how many files of each kind there are\n",
    "2. Filter by files ending in `.csv` and have years `2005-2015`\n",
    "3. Download the data from GitHub\n",
    "4. Create a folder called `pbp_csv` in working dir, and save retrieved CSV files there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30f7b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total assets in release 'pbp': 160\n",
      "CSV assets detected in seasons 2005–2015: 11\n",
      " - play_by_play_2005.csv\n",
      " - play_by_play_2006.csv\n",
      " - play_by_play_2007.csv\n",
      " - play_by_play_2008.csv\n",
      " - play_by_play_2009.csv\n",
      " - play_by_play_2010.csv\n",
      " - play_by_play_2011.csv\n",
      " - play_by_play_2012.csv\n",
      " - play_by_play_2013.csv\n",
      " - play_by_play_2014.csv\n",
      " - play_by_play_2015.csv\n",
      "Downloading play_by_play_2005.csv -> pbp_csv/play_by_play_2005.csv\n",
      "Finished play_by_play_2005.csv\n",
      "\n",
      "Downloading play_by_play_2006.csv -> pbp_csv/play_by_play_2006.csv\n",
      "Finished play_by_play_2006.csv\n",
      "\n",
      "Downloading play_by_play_2007.csv -> pbp_csv/play_by_play_2007.csv\n",
      "Finished play_by_play_2007.csv\n",
      "\n",
      "Downloading play_by_play_2008.csv -> pbp_csv/play_by_play_2008.csv\n",
      "Finished play_by_play_2008.csv\n",
      "\n",
      "Downloading play_by_play_2009.csv -> pbp_csv/play_by_play_2009.csv\n",
      "Finished play_by_play_2009.csv\n",
      "\n",
      "Downloading play_by_play_2010.csv -> pbp_csv/play_by_play_2010.csv\n",
      "Finished play_by_play_2010.csv\n",
      "\n",
      "Downloading play_by_play_2011.csv -> pbp_csv/play_by_play_2011.csv\n",
      "Finished play_by_play_2011.csv\n",
      "\n",
      "Downloading play_by_play_2012.csv -> pbp_csv/play_by_play_2012.csv\n",
      "Finished play_by_play_2012.csv\n",
      "\n",
      "Downloading play_by_play_2013.csv -> pbp_csv/play_by_play_2013.csv\n",
      "Finished play_by_play_2013.csv\n",
      "\n",
      "Downloading play_by_play_2014.csv -> pbp_csv/play_by_play_2014.csv\n",
      "Finished play_by_play_2014.csv\n",
      "\n",
      "Downloading play_by_play_2015.csv -> pbp_csv/play_by_play_2015.csv\n",
      "Finished play_by_play_2015.csv\n",
      "\n",
      "All requested downloads complete.\n"
     ]
    }
   ],
   "source": [
    "# Fetch Release Metadata\n",
    "\n",
    "release = fetch_release(TAG)\n",
    "assets = release.get(\"assets\", [])\n",
    "\n",
    "# print statements to keep track and help with debugging\n",
    "print(f\"Total assets in release '{TAG}': {len(assets)}\")\n",
    "\n",
    "# Filter Assets by .csv Extension and Year Range\n",
    "\n",
    "csv_assets = [a for a in assets if a.get(\"name\", \"\").endswith(\".csv\")]\n",
    "\n",
    "filtered_assets = []\n",
    "for asset in csv_assets:\n",
    "    name = asset.get(\"name\", \"\")\n",
    "    year = extract_year_from_filename(name)\n",
    "    if year is not None and START_SEASON <= year <= END_SEASON:\n",
    "        filtered_assets.append(asset)\n",
    "\n",
    "print(f\"CSV assets detected in seasons {START_SEASON}–{END_SEASON}: {len(filtered_assets)}\")\n",
    "for a in filtered_assets:\n",
    "    print(\" -\", a[\"name\"])\n",
    "\n",
    "# Download Selected Assets\n",
    "\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "for asset in filtered_assets:\n",
    "    name = asset[\"name\"]\n",
    "    url = asset[\"browser_download_url\"]\n",
    "    out_path = os.path.join(DOWNLOAD_DIR, name)\n",
    "\n",
    "    # constant print statement to keep track of progress and potential debugging\n",
    "    print(f\"Downloading {name} -> {out_path}\")\n",
    "    download_asset(url, out_path)\n",
    "    print(f\"Finished {name}\\n\")\n",
    "\n",
    "print(\"All requested downloads complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a348ee",
   "metadata": {},
   "source": [
    "This notebook is only meant to be run once to create the `pbp_csv` folder and load it with the 10 CSV files.\\\n",
    "Once that has been created, you may move on to ***Notebook-2*** where subsequent data cleaning, manipulation and analysis is done. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
